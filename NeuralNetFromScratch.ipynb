{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_!.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6eolshhpdPnVmLY9VQUv9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzS3CA3rZpbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOYgSRTlXH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(([2,9],[1,5],[3,6]),dtype=torch.float) # 3x2 tensor\n",
        "y = torch.tensor(([92],[100],[89]),dtype=torch.float) # 3X1 tensor\n",
        "xPredicted = torch.tensor(([4,8]),dtype=torch.float) # 2X1 tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRjKpYgANtxi",
        "colab_type": "code",
        "outputId": "706312d4-ddb8-492c-afec-92f43a1b4313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X.size())\n",
        "print(y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUB1rl1aNxOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scale units\n",
        "X_max, _ = torch.max(X,0) # dummy variable to capture and drop indices (we only need max value)\n",
        "xPredicted_max, _ = torch.max(xPredicted,0)\n",
        "#Dividing tensors\n",
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted,xPredicted_max)\n",
        "y = y /100 # max test score is 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOKOdLCtN2co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super(Neural_Network,self).__init__()\n",
        "    #Parametres\n",
        "    #TODO: parameters can be parameterized instead of declaring them here\n",
        "    self.inputSize = 2\n",
        "    self.outputSize = 1\n",
        "    self.hiddenSize = 3\n",
        "    self.intermediate = 3\n",
        "\n",
        "    #weights\n",
        "    self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 2X3 tensor\n",
        "    self.WI = torch.randn(self.hiddenSize,self.intermediate) # 3x3 tensor \n",
        "    self.W2 = torch.randn(self.intermediate,self.outputSize) # 3x1 tensor\n",
        "\n",
        "  \n",
        "  def forward(self, X):\n",
        "    self.z = torch.matmul(X,self.W1) #3x3\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.ZI = torch.matmul(self.z2,self.WI)\n",
        "    self.zi1 = self.sigmoid(self.ZI)\n",
        "    self.z3 = torch.matmul(self.zi1, self.W2)\n",
        "    o = self.sigmoid(self.z3) # final activation\n",
        "    return o\n",
        "  \n",
        "  def sigmoid(self,s):\n",
        "    return 1 / (1+torch.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self,s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    self.o_error = y - 0 #error in output\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sigmoid to error\n",
        "    self.ZI_error = torch.matmul(self.o_delta,torch.t(self.W2))\n",
        "    self.ZI_delta = self.ZI_error * self.sigmoidPrime(self.ZI) # derivative of inter hidden layer\n",
        "    self.z2_error = torch.matmul(self.ZI_delta,torch.t(self.WI)) # error of 1 hidden layer\n",
        "    self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) # derivative of 1 hidden layer\n",
        "    self.W1 += torch.matmul(torch.t(X), self.z2_delta) \n",
        "    self.WI += torch.matmul(torch.t(self.z2),self.ZI_delta)\n",
        "    self.W2 += torch.matmul(torch.t(self.zi1), self.o_delta)\n",
        "\n",
        "  def train(self, X,y):\n",
        "    #forward + backward pass for training\n",
        "    o = self.forward(X)\n",
        "    self.backward(X,y,o)\n",
        "\n",
        "  def saveWeights(self,model):\n",
        "    #we will use the pytorch internal storage function\n",
        "    torch.save(model, 'NN')\n",
        "    # you can reload model with all the weights and so forth with:\n",
        "    # torch.load('NN')\n",
        "\n",
        "  def predict(self):\n",
        "    print('Predicted data based on trained weights :')\n",
        "    print('Input (scaled): \\n' + str(xPredicted))\n",
        "    print('Output: \\n'+ str(self.forward(xPredicted)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFf0k1VoPgXI",
        "colab_type": "code",
        "outputId": "c861fb44-8302-4516-a6e8-5d481199052f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "torch.mean((y-NN(X))**2).detach().item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1965477019548416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMKQ1es38jl",
        "colab_type": "code",
        "outputId": "d01605fd-63e1-44b1-a75c-fddc1122d53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(1000):\n",
        "  print('#'+str(i)+\" Loss: \" + str(torch.mean((y-NN(X))**2).detach().item()))\n",
        "  NN.train(X,y)\n",
        "NN.saveWeights(NN)\n",
        "NN.predict()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.1730843186378479\n",
            "#1 Loss: 0.17422181367874146\n",
            "#2 Loss: 0.17535604536533356\n",
            "#3 Loss: 0.1764867752790451\n",
            "#4 Loss: 0.17761355638504028\n",
            "#5 Loss: 0.1787361353635788\n",
            "#6 Loss: 0.17985416948795319\n",
            "#7 Loss: 0.1809672862291336\n",
            "#8 Loss: 0.18207518756389618\n",
            "#9 Loss: 0.1831774264574051\n",
            "#10 Loss: 0.18427376449108124\n",
            "#11 Loss: 0.18536365032196045\n",
            "#12 Loss: 0.18644684553146362\n",
            "#13 Loss: 0.18752288818359375\n",
            "#14 Loss: 0.1885913610458374\n",
            "#15 Loss: 0.18965189158916473\n",
            "#16 Loss: 0.19070406258106232\n",
            "#17 Loss: 0.19174735248088837\n",
            "#18 Loss: 0.19278137385845184\n",
            "#19 Loss: 0.19380563497543335\n",
            "#20 Loss: 0.19481973350048065\n",
            "#21 Loss: 0.19582311809062958\n",
            "#22 Loss: 0.19681526720523834\n",
            "#23 Loss: 0.1977958083152771\n",
            "#24 Loss: 0.19876402616500854\n",
            "#25 Loss: 0.19971950352191925\n",
            "#26 Loss: 0.20066170394420624\n",
            "#27 Loss: 0.20159001648426056\n",
            "#28 Loss: 0.20250384509563446\n",
            "#29 Loss: 0.20340262353420258\n",
            "#30 Loss: 0.20428572595119476\n",
            "#31 Loss: 0.2051525115966797\n",
            "#32 Loss: 0.2060023546218872\n",
            "#33 Loss: 0.2068345695734024\n",
            "#34 Loss: 0.20764847099781036\n",
            "#35 Loss: 0.20844335854053497\n",
            "#36 Loss: 0.20921850204467773\n",
            "#37 Loss: 0.20997314155101776\n",
            "#38 Loss: 0.21070651710033417\n",
            "#39 Loss: 0.21141783893108368\n",
            "#40 Loss: 0.21210630238056183\n",
            "#41 Loss: 0.21277105808258057\n",
            "#42 Loss: 0.21341127157211304\n",
            "#43 Loss: 0.21402597427368164\n",
            "#44 Loss: 0.21461431682109833\n",
            "#45 Loss: 0.2151753455400467\n",
            "#46 Loss: 0.21570807695388794\n",
            "#47 Loss: 0.21621154248714447\n",
            "#48 Loss: 0.21668469905853271\n",
            "#49 Loss: 0.2171265035867691\n",
            "#50 Loss: 0.21753591299057007\n",
            "#51 Loss: 0.21791179478168488\n",
            "#52 Loss: 0.21825306117534637\n",
            "#53 Loss: 0.21855849027633667\n",
            "#54 Loss: 0.21882694959640503\n",
            "#55 Loss: 0.21905727684497833\n",
            "#56 Loss: 0.2192482203245163\n",
            "#57 Loss: 0.21939855813980103\n",
            "#58 Loss: 0.2195071130990982\n",
            "#59 Loss: 0.21957261860370636\n",
            "#60 Loss: 0.2195938676595688\n",
            "#61 Loss: 0.21956956386566162\n",
            "#62 Loss: 0.2194986194372177\n",
            "#63 Loss: 0.21937985718250275\n",
            "#64 Loss: 0.21921218931674957\n",
            "#65 Loss: 0.2189946174621582\n",
            "#66 Loss: 0.21872621774673462\n",
            "#67 Loss: 0.21840624511241913\n",
            "#68 Loss: 0.21803416311740875\n",
            "#69 Loss: 0.2176094800233841\n",
            "#70 Loss: 0.21713216602802277\n",
            "#71 Loss: 0.21660250425338745\n",
            "#72 Loss: 0.2160210758447647\n",
            "#73 Loss: 0.21538905799388885\n",
            "#74 Loss: 0.21470828354358673\n",
            "#75 Loss: 0.21398131549358368\n",
            "#76 Loss: 0.2132115364074707\n",
            "#77 Loss: 0.2124035358428955\n",
            "#78 Loss: 0.21156316995620728\n",
            "#79 Loss: 0.21069782972335815\n",
            "#80 Loss: 0.20981676876544952\n",
            "#81 Loss: 0.2089313119649887\n",
            "#82 Loss: 0.2080552726984024\n",
            "#83 Loss: 0.20720522105693817\n",
            "#84 Loss: 0.2064008265733719\n",
            "#85 Loss: 0.20566511154174805\n",
            "#86 Loss: 0.20502425730228424\n",
            "#87 Loss: 0.20450778305530548\n",
            "#88 Loss: 0.20414765179157257\n",
            "#89 Loss: 0.2039770931005478\n",
            "#90 Loss: 0.20402848720550537\n",
            "#91 Loss: 0.20433087646961212\n",
            "#92 Loss: 0.20490652322769165\n",
            "#93 Loss: 0.20576860010623932\n",
            "#94 Loss: 0.20692265033721924\n",
            "#95 Loss: 0.2083788365125656\n",
            "#96 Loss: 0.21018852293491364\n",
            "#97 Loss: 0.21254396438598633\n",
            "#98 Loss: 0.21608145534992218\n",
            "#99 Loss: 0.2227574586868286\n",
            "#100 Loss: 0.232712522149086\n",
            "#101 Loss: 0.23631416261196136\n",
            "#102 Loss: 0.23828060925006866\n",
            "#103 Loss: 0.24012519419193268\n",
            "#104 Loss: 0.2419573813676834\n",
            "#105 Loss: 0.24378912150859833\n",
            "#106 Loss: 0.24562013149261475\n",
            "#107 Loss: 0.2474508136510849\n",
            "#108 Loss: nan\n",
            "#109 Loss: nan\n",
            "#110 Loss: nan\n",
            "#111 Loss: nan\n",
            "#112 Loss: nan\n",
            "#113 Loss: nan\n",
            "#114 Loss: nan\n",
            "#115 Loss: nan\n",
            "#116 Loss: nan\n",
            "#117 Loss: nan\n",
            "#118 Loss: nan\n",
            "#119 Loss: nan\n",
            "#120 Loss: nan\n",
            "#121 Loss: nan\n",
            "#122 Loss: nan\n",
            "#123 Loss: nan\n",
            "#124 Loss: nan\n",
            "#125 Loss: nan\n",
            "#126 Loss: nan\n",
            "#127 Loss: nan\n",
            "#128 Loss: nan\n",
            "#129 Loss: nan\n",
            "#130 Loss: nan\n",
            "#131 Loss: nan\n",
            "#132 Loss: nan\n",
            "#133 Loss: nan\n",
            "#134 Loss: nan\n",
            "#135 Loss: nan\n",
            "#136 Loss: nan\n",
            "#137 Loss: nan\n",
            "#138 Loss: nan\n",
            "#139 Loss: nan\n",
            "#140 Loss: nan\n",
            "#141 Loss: nan\n",
            "#142 Loss: nan\n",
            "#143 Loss: nan\n",
            "#144 Loss: nan\n",
            "#145 Loss: nan\n",
            "#146 Loss: nan\n",
            "#147 Loss: nan\n",
            "#148 Loss: nan\n",
            "#149 Loss: nan\n",
            "#150 Loss: nan\n",
            "#151 Loss: nan\n",
            "#152 Loss: nan\n",
            "#153 Loss: nan\n",
            "#154 Loss: nan\n",
            "#155 Loss: nan\n",
            "#156 Loss: nan\n",
            "#157 Loss: nan\n",
            "#158 Loss: nan\n",
            "#159 Loss: nan\n",
            "#160 Loss: nan\n",
            "#161 Loss: nan\n",
            "#162 Loss: nan\n",
            "#163 Loss: nan\n",
            "#164 Loss: nan\n",
            "#165 Loss: nan\n",
            "#166 Loss: nan\n",
            "#167 Loss: nan\n",
            "#168 Loss: nan\n",
            "#169 Loss: nan\n",
            "#170 Loss: nan\n",
            "#171 Loss: nan\n",
            "#172 Loss: nan\n",
            "#173 Loss: nan\n",
            "#174 Loss: nan\n",
            "#175 Loss: nan\n",
            "#176 Loss: nan\n",
            "#177 Loss: nan\n",
            "#178 Loss: nan\n",
            "#179 Loss: nan\n",
            "#180 Loss: nan\n",
            "#181 Loss: nan\n",
            "#182 Loss: nan\n",
            "#183 Loss: nan\n",
            "#184 Loss: nan\n",
            "#185 Loss: nan\n",
            "#186 Loss: nan\n",
            "#187 Loss: nan\n",
            "#188 Loss: nan\n",
            "#189 Loss: nan\n",
            "#190 Loss: nan\n",
            "#191 Loss: nan\n",
            "#192 Loss: nan\n",
            "#193 Loss: nan\n",
            "#194 Loss: nan\n",
            "#195 Loss: nan\n",
            "#196 Loss: nan\n",
            "#197 Loss: nan\n",
            "#198 Loss: nan\n",
            "#199 Loss: nan\n",
            "#200 Loss: nan\n",
            "#201 Loss: nan\n",
            "#202 Loss: nan\n",
            "#203 Loss: nan\n",
            "#204 Loss: nan\n",
            "#205 Loss: nan\n",
            "#206 Loss: nan\n",
            "#207 Loss: nan\n",
            "#208 Loss: nan\n",
            "#209 Loss: nan\n",
            "#210 Loss: nan\n",
            "#211 Loss: nan\n",
            "#212 Loss: nan\n",
            "#213 Loss: nan\n",
            "#214 Loss: nan\n",
            "#215 Loss: nan\n",
            "#216 Loss: nan\n",
            "#217 Loss: nan\n",
            "#218 Loss: nan\n",
            "#219 Loss: nan\n",
            "#220 Loss: nan\n",
            "#221 Loss: nan\n",
            "#222 Loss: nan\n",
            "#223 Loss: nan\n",
            "#224 Loss: nan\n",
            "#225 Loss: nan\n",
            "#226 Loss: nan\n",
            "#227 Loss: nan\n",
            "#228 Loss: nan\n",
            "#229 Loss: nan\n",
            "#230 Loss: nan\n",
            "#231 Loss: nan\n",
            "#232 Loss: nan\n",
            "#233 Loss: nan\n",
            "#234 Loss: nan\n",
            "#235 Loss: nan\n",
            "#236 Loss: nan\n",
            "#237 Loss: nan\n",
            "#238 Loss: nan\n",
            "#239 Loss: nan\n",
            "#240 Loss: nan\n",
            "#241 Loss: nan\n",
            "#242 Loss: nan\n",
            "#243 Loss: nan\n",
            "#244 Loss: nan\n",
            "#245 Loss: nan\n",
            "#246 Loss: nan\n",
            "#247 Loss: nan\n",
            "#248 Loss: nan\n",
            "#249 Loss: nan\n",
            "#250 Loss: nan\n",
            "#251 Loss: nan\n",
            "#252 Loss: nan\n",
            "#253 Loss: nan\n",
            "#254 Loss: nan\n",
            "#255 Loss: nan\n",
            "#256 Loss: nan\n",
            "#257 Loss: nan\n",
            "#258 Loss: nan\n",
            "#259 Loss: nan\n",
            "#260 Loss: nan\n",
            "#261 Loss: nan\n",
            "#262 Loss: nan\n",
            "#263 Loss: nan\n",
            "#264 Loss: nan\n",
            "#265 Loss: nan\n",
            "#266 Loss: nan\n",
            "#267 Loss: nan\n",
            "#268 Loss: nan\n",
            "#269 Loss: nan\n",
            "#270 Loss: nan\n",
            "#271 Loss: nan\n",
            "#272 Loss: nan\n",
            "#273 Loss: nan\n",
            "#274 Loss: nan\n",
            "#275 Loss: nan\n",
            "#276 Loss: nan\n",
            "#277 Loss: nan\n",
            "#278 Loss: nan\n",
            "#279 Loss: nan\n",
            "#280 Loss: nan\n",
            "#281 Loss: nan\n",
            "#282 Loss: nan\n",
            "#283 Loss: nan\n",
            "#284 Loss: nan\n",
            "#285 Loss: nan\n",
            "#286 Loss: nan\n",
            "#287 Loss: nan\n",
            "#288 Loss: nan\n",
            "#289 Loss: nan\n",
            "#290 Loss: nan\n",
            "#291 Loss: nan\n",
            "#292 Loss: nan\n",
            "#293 Loss: nan\n",
            "#294 Loss: nan\n",
            "#295 Loss: nan\n",
            "#296 Loss: nan\n",
            "#297 Loss: nan\n",
            "#298 Loss: nan\n",
            "#299 Loss: nan\n",
            "#300 Loss: nan\n",
            "#301 Loss: nan\n",
            "#302 Loss: nan\n",
            "#303 Loss: nan\n",
            "#304 Loss: nan\n",
            "#305 Loss: nan\n",
            "#306 Loss: nan\n",
            "#307 Loss: nan\n",
            "#308 Loss: nan\n",
            "#309 Loss: nan\n",
            "#310 Loss: nan\n",
            "#311 Loss: nan\n",
            "#312 Loss: nan\n",
            "#313 Loss: nan\n",
            "#314 Loss: nan\n",
            "#315 Loss: nan\n",
            "#316 Loss: nan\n",
            "#317 Loss: nan\n",
            "#318 Loss: nan\n",
            "#319 Loss: nan\n",
            "#320 Loss: nan\n",
            "#321 Loss: nan\n",
            "#322 Loss: nan\n",
            "#323 Loss: nan\n",
            "#324 Loss: nan\n",
            "#325 Loss: nan\n",
            "#326 Loss: nan\n",
            "#327 Loss: nan\n",
            "#328 Loss: nan\n",
            "#329 Loss: nan\n",
            "#330 Loss: nan\n",
            "#331 Loss: nan\n",
            "#332 Loss: nan\n",
            "#333 Loss: nan\n",
            "#334 Loss: nan\n",
            "#335 Loss: nan\n",
            "#336 Loss: nan\n",
            "#337 Loss: nan\n",
            "#338 Loss: nan\n",
            "#339 Loss: nan\n",
            "#340 Loss: nan\n",
            "#341 Loss: nan\n",
            "#342 Loss: nan\n",
            "#343 Loss: nan\n",
            "#344 Loss: nan\n",
            "#345 Loss: nan\n",
            "#346 Loss: nan\n",
            "#347 Loss: nan\n",
            "#348 Loss: nan\n",
            "#349 Loss: nan\n",
            "#350 Loss: nan\n",
            "#351 Loss: nan\n",
            "#352 Loss: nan\n",
            "#353 Loss: nan\n",
            "#354 Loss: nan\n",
            "#355 Loss: nan\n",
            "#356 Loss: nan\n",
            "#357 Loss: nan\n",
            "#358 Loss: nan\n",
            "#359 Loss: nan\n",
            "#360 Loss: nan\n",
            "#361 Loss: nan\n",
            "#362 Loss: nan\n",
            "#363 Loss: nan\n",
            "#364 Loss: nan\n",
            "#365 Loss: nan\n",
            "#366 Loss: nan\n",
            "#367 Loss: nan\n",
            "#368 Loss: nan\n",
            "#369 Loss: nan\n",
            "#370 Loss: nan\n",
            "#371 Loss: nan\n",
            "#372 Loss: nan\n",
            "#373 Loss: nan\n",
            "#374 Loss: nan\n",
            "#375 Loss: nan\n",
            "#376 Loss: nan\n",
            "#377 Loss: nan\n",
            "#378 Loss: nan\n",
            "#379 Loss: nan\n",
            "#380 Loss: nan\n",
            "#381 Loss: nan\n",
            "#382 Loss: nan\n",
            "#383 Loss: nan\n",
            "#384 Loss: nan\n",
            "#385 Loss: nan\n",
            "#386 Loss: nan\n",
            "#387 Loss: nan\n",
            "#388 Loss: nan\n",
            "#389 Loss: nan\n",
            "#390 Loss: nan\n",
            "#391 Loss: nan\n",
            "#392 Loss: nan\n",
            "#393 Loss: nan\n",
            "#394 Loss: nan\n",
            "#395 Loss: nan\n",
            "#396 Loss: nan\n",
            "#397 Loss: nan\n",
            "#398 Loss: nan\n",
            "#399 Loss: nan\n",
            "#400 Loss: nan\n",
            "#401 Loss: nan\n",
            "#402 Loss: nan\n",
            "#403 Loss: nan\n",
            "#404 Loss: nan\n",
            "#405 Loss: nan\n",
            "#406 Loss: nan\n",
            "#407 Loss: nan\n",
            "#408 Loss: nan\n",
            "#409 Loss: nan\n",
            "#410 Loss: nan\n",
            "#411 Loss: nan\n",
            "#412 Loss: nan\n",
            "#413 Loss: nan\n",
            "#414 Loss: nan\n",
            "#415 Loss: nan\n",
            "#416 Loss: nan\n",
            "#417 Loss: nan\n",
            "#418 Loss: nan\n",
            "#419 Loss: nan\n",
            "#420 Loss: nan\n",
            "#421 Loss: nan\n",
            "#422 Loss: nan\n",
            "#423 Loss: nan\n",
            "#424 Loss: nan\n",
            "#425 Loss: nan\n",
            "#426 Loss: nan\n",
            "#427 Loss: nan\n",
            "#428 Loss: nan\n",
            "#429 Loss: nan\n",
            "#430 Loss: nan\n",
            "#431 Loss: nan\n",
            "#432 Loss: nan\n",
            "#433 Loss: nan\n",
            "#434 Loss: nan\n",
            "#435 Loss: nan\n",
            "#436 Loss: nan\n",
            "#437 Loss: nan\n",
            "#438 Loss: nan\n",
            "#439 Loss: nan\n",
            "#440 Loss: nan\n",
            "#441 Loss: nan\n",
            "#442 Loss: nan\n",
            "#443 Loss: nan\n",
            "#444 Loss: nan\n",
            "#445 Loss: nan\n",
            "#446 Loss: nan\n",
            "#447 Loss: nan\n",
            "#448 Loss: nan\n",
            "#449 Loss: nan\n",
            "#450 Loss: nan\n",
            "#451 Loss: nan\n",
            "#452 Loss: nan\n",
            "#453 Loss: nan\n",
            "#454 Loss: nan\n",
            "#455 Loss: nan\n",
            "#456 Loss: nan\n",
            "#457 Loss: nan\n",
            "#458 Loss: nan\n",
            "#459 Loss: nan\n",
            "#460 Loss: nan\n",
            "#461 Loss: nan\n",
            "#462 Loss: nan\n",
            "#463 Loss: nan\n",
            "#464 Loss: nan\n",
            "#465 Loss: nan\n",
            "#466 Loss: nan\n",
            "#467 Loss: nan\n",
            "#468 Loss: nan\n",
            "#469 Loss: nan\n",
            "#470 Loss: nan\n",
            "#471 Loss: nan\n",
            "#472 Loss: nan\n",
            "#473 Loss: nan\n",
            "#474 Loss: nan\n",
            "#475 Loss: nan\n",
            "#476 Loss: nan\n",
            "#477 Loss: nan\n",
            "#478 Loss: nan\n",
            "#479 Loss: nan\n",
            "#480 Loss: nan\n",
            "#481 Loss: nan\n",
            "#482 Loss: nan\n",
            "#483 Loss: nan\n",
            "#484 Loss: nan\n",
            "#485 Loss: nan\n",
            "#486 Loss: nan\n",
            "#487 Loss: nan\n",
            "#488 Loss: nan\n",
            "#489 Loss: nan\n",
            "#490 Loss: nan\n",
            "#491 Loss: nan\n",
            "#492 Loss: nan\n",
            "#493 Loss: nan\n",
            "#494 Loss: nan\n",
            "#495 Loss: nan\n",
            "#496 Loss: nan\n",
            "#497 Loss: nan\n",
            "#498 Loss: nan\n",
            "#499 Loss: nan\n",
            "#500 Loss: nan\n",
            "#501 Loss: nan\n",
            "#502 Loss: nan\n",
            "#503 Loss: nan\n",
            "#504 Loss: nan\n",
            "#505 Loss: nan\n",
            "#506 Loss: nan\n",
            "#507 Loss: nan\n",
            "#508 Loss: nan\n",
            "#509 Loss: nan\n",
            "#510 Loss: nan\n",
            "#511 Loss: nan\n",
            "#512 Loss: nan\n",
            "#513 Loss: nan\n",
            "#514 Loss: nan\n",
            "#515 Loss: nan\n",
            "#516 Loss: nan\n",
            "#517 Loss: nan\n",
            "#518 Loss: nan\n",
            "#519 Loss: nan\n",
            "#520 Loss: nan\n",
            "#521 Loss: nan\n",
            "#522 Loss: nan\n",
            "#523 Loss: nan\n",
            "#524 Loss: nan\n",
            "#525 Loss: nan\n",
            "#526 Loss: nan\n",
            "#527 Loss: nan\n",
            "#528 Loss: nan\n",
            "#529 Loss: nan\n",
            "#530 Loss: nan\n",
            "#531 Loss: nan\n",
            "#532 Loss: nan\n",
            "#533 Loss: nan\n",
            "#534 Loss: nan\n",
            "#535 Loss: nan\n",
            "#536 Loss: nan\n",
            "#537 Loss: nan\n",
            "#538 Loss: nan\n",
            "#539 Loss: nan\n",
            "#540 Loss: nan\n",
            "#541 Loss: nan\n",
            "#542 Loss: nan\n",
            "#543 Loss: nan\n",
            "#544 Loss: nan\n",
            "#545 Loss: nan\n",
            "#546 Loss: nan\n",
            "#547 Loss: nan\n",
            "#548 Loss: nan\n",
            "#549 Loss: nan\n",
            "#550 Loss: nan\n",
            "#551 Loss: nan\n",
            "#552 Loss: nan\n",
            "#553 Loss: nan\n",
            "#554 Loss: nan\n",
            "#555 Loss: nan\n",
            "#556 Loss: nan\n",
            "#557 Loss: nan\n",
            "#558 Loss: nan\n",
            "#559 Loss: nan\n",
            "#560 Loss: nan\n",
            "#561 Loss: nan\n",
            "#562 Loss: nan\n",
            "#563 Loss: nan\n",
            "#564 Loss: nan\n",
            "#565 Loss: nan\n",
            "#566 Loss: nan\n",
            "#567 Loss: nan\n",
            "#568 Loss: nan\n",
            "#569 Loss: nan\n",
            "#570 Loss: nan\n",
            "#571 Loss: nan\n",
            "#572 Loss: nan\n",
            "#573 Loss: nan\n",
            "#574 Loss: nan\n",
            "#575 Loss: nan\n",
            "#576 Loss: nan\n",
            "#577 Loss: nan\n",
            "#578 Loss: nan\n",
            "#579 Loss: nan\n",
            "#580 Loss: nan\n",
            "#581 Loss: nan\n",
            "#582 Loss: nan\n",
            "#583 Loss: nan\n",
            "#584 Loss: nan\n",
            "#585 Loss: nan\n",
            "#586 Loss: nan\n",
            "#587 Loss: nan\n",
            "#588 Loss: nan\n",
            "#589 Loss: nan\n",
            "#590 Loss: nan\n",
            "#591 Loss: nan\n",
            "#592 Loss: nan\n",
            "#593 Loss: nan\n",
            "#594 Loss: nan\n",
            "#595 Loss: nan\n",
            "#596 Loss: nan\n",
            "#597 Loss: nan\n",
            "#598 Loss: nan\n",
            "#599 Loss: nan\n",
            "#600 Loss: nan\n",
            "#601 Loss: nan\n",
            "#602 Loss: nan\n",
            "#603 Loss: nan\n",
            "#604 Loss: nan\n",
            "#605 Loss: nan\n",
            "#606 Loss: nan\n",
            "#607 Loss: nan\n",
            "#608 Loss: nan\n",
            "#609 Loss: nan\n",
            "#610 Loss: nan\n",
            "#611 Loss: nan\n",
            "#612 Loss: nan\n",
            "#613 Loss: nan\n",
            "#614 Loss: nan\n",
            "#615 Loss: nan\n",
            "#616 Loss: nan\n",
            "#617 Loss: nan\n",
            "#618 Loss: nan\n",
            "#619 Loss: nan\n",
            "#620 Loss: nan\n",
            "#621 Loss: nan\n",
            "#622 Loss: nan\n",
            "#623 Loss: nan\n",
            "#624 Loss: nan\n",
            "#625 Loss: nan\n",
            "#626 Loss: nan\n",
            "#627 Loss: nan\n",
            "#628 Loss: nan\n",
            "#629 Loss: nan\n",
            "#630 Loss: nan\n",
            "#631 Loss: nan\n",
            "#632 Loss: nan\n",
            "#633 Loss: nan\n",
            "#634 Loss: nan\n",
            "#635 Loss: nan\n",
            "#636 Loss: nan\n",
            "#637 Loss: nan\n",
            "#638 Loss: nan\n",
            "#639 Loss: nan\n",
            "#640 Loss: nan\n",
            "#641 Loss: nan\n",
            "#642 Loss: nan\n",
            "#643 Loss: nan\n",
            "#644 Loss: nan\n",
            "#645 Loss: nan\n",
            "#646 Loss: nan\n",
            "#647 Loss: nan\n",
            "#648 Loss: nan\n",
            "#649 Loss: nan\n",
            "#650 Loss: nan\n",
            "#651 Loss: nan\n",
            "#652 Loss: nan\n",
            "#653 Loss: nan\n",
            "#654 Loss: nan\n",
            "#655 Loss: nan\n",
            "#656 Loss: nan\n",
            "#657 Loss: nan\n",
            "#658 Loss: nan\n",
            "#659 Loss: nan\n",
            "#660 Loss: nan\n",
            "#661 Loss: nan\n",
            "#662 Loss: nan\n",
            "#663 Loss: nan\n",
            "#664 Loss: nan\n",
            "#665 Loss: nan\n",
            "#666 Loss: nan\n",
            "#667 Loss: nan\n",
            "#668 Loss: nan\n",
            "#669 Loss: nan\n",
            "#670 Loss: nan\n",
            "#671 Loss: nan\n",
            "#672 Loss: nan\n",
            "#673 Loss: nan\n",
            "#674 Loss: nan\n",
            "#675 Loss: nan\n",
            "#676 Loss: nan\n",
            "#677 Loss: nan\n",
            "#678 Loss: nan\n",
            "#679 Loss: nan\n",
            "#680 Loss: nan\n",
            "#681 Loss: nan\n",
            "#682 Loss: nan\n",
            "#683 Loss: nan\n",
            "#684 Loss: nan\n",
            "#685 Loss: nan\n",
            "#686 Loss: nan\n",
            "#687 Loss: nan\n",
            "#688 Loss: nan\n",
            "#689 Loss: nan\n",
            "#690 Loss: nan\n",
            "#691 Loss: nan\n",
            "#692 Loss: nan\n",
            "#693 Loss: nan\n",
            "#694 Loss: nan\n",
            "#695 Loss: nan\n",
            "#696 Loss: nan\n",
            "#697 Loss: nan\n",
            "#698 Loss: nan\n",
            "#699 Loss: nan\n",
            "#700 Loss: nan\n",
            "#701 Loss: nan\n",
            "#702 Loss: nan\n",
            "#703 Loss: nan\n",
            "#704 Loss: nan\n",
            "#705 Loss: nan\n",
            "#706 Loss: nan\n",
            "#707 Loss: nan\n",
            "#708 Loss: nan\n",
            "#709 Loss: nan\n",
            "#710 Loss: nan\n",
            "#711 Loss: nan\n",
            "#712 Loss: nan\n",
            "#713 Loss: nan\n",
            "#714 Loss: nan\n",
            "#715 Loss: nan\n",
            "#716 Loss: nan\n",
            "#717 Loss: nan\n",
            "#718 Loss: nan\n",
            "#719 Loss: nan\n",
            "#720 Loss: nan\n",
            "#721 Loss: nan\n",
            "#722 Loss: nan\n",
            "#723 Loss: nan\n",
            "#724 Loss: nan\n",
            "#725 Loss: nan\n",
            "#726 Loss: nan\n",
            "#727 Loss: nan\n",
            "#728 Loss: nan\n",
            "#729 Loss: nan\n",
            "#730 Loss: nan\n",
            "#731 Loss: nan\n",
            "#732 Loss: nan\n",
            "#733 Loss: nan\n",
            "#734 Loss: nan\n",
            "#735 Loss: nan\n",
            "#736 Loss: nan\n",
            "#737 Loss: nan\n",
            "#738 Loss: nan\n",
            "#739 Loss: nan\n",
            "#740 Loss: nan\n",
            "#741 Loss: nan\n",
            "#742 Loss: nan\n",
            "#743 Loss: nan\n",
            "#744 Loss: nan\n",
            "#745 Loss: nan\n",
            "#746 Loss: nan\n",
            "#747 Loss: nan\n",
            "#748 Loss: nan\n",
            "#749 Loss: nan\n",
            "#750 Loss: nan\n",
            "#751 Loss: nan\n",
            "#752 Loss: nan\n",
            "#753 Loss: nan\n",
            "#754 Loss: nan\n",
            "#755 Loss: nan\n",
            "#756 Loss: nan\n",
            "#757 Loss: nan\n",
            "#758 Loss: nan\n",
            "#759 Loss: nan\n",
            "#760 Loss: nan\n",
            "#761 Loss: nan\n",
            "#762 Loss: nan\n",
            "#763 Loss: nan\n",
            "#764 Loss: nan\n",
            "#765 Loss: nan\n",
            "#766 Loss: nan\n",
            "#767 Loss: nan\n",
            "#768 Loss: nan\n",
            "#769 Loss: nan\n",
            "#770 Loss: nan\n",
            "#771 Loss: nan\n",
            "#772 Loss: nan\n",
            "#773 Loss: nan\n",
            "#774 Loss: nan\n",
            "#775 Loss: nan\n",
            "#776 Loss: nan\n",
            "#777 Loss: nan\n",
            "#778 Loss: nan\n",
            "#779 Loss: nan\n",
            "#780 Loss: nan\n",
            "#781 Loss: nan\n",
            "#782 Loss: nan\n",
            "#783 Loss: nan\n",
            "#784 Loss: nan\n",
            "#785 Loss: nan\n",
            "#786 Loss: nan\n",
            "#787 Loss: nan\n",
            "#788 Loss: nan\n",
            "#789 Loss: nan\n",
            "#790 Loss: nan\n",
            "#791 Loss: nan\n",
            "#792 Loss: nan\n",
            "#793 Loss: nan\n",
            "#794 Loss: nan\n",
            "#795 Loss: nan\n",
            "#796 Loss: nan\n",
            "#797 Loss: nan\n",
            "#798 Loss: nan\n",
            "#799 Loss: nan\n",
            "#800 Loss: nan\n",
            "#801 Loss: nan\n",
            "#802 Loss: nan\n",
            "#803 Loss: nan\n",
            "#804 Loss: nan\n",
            "#805 Loss: nan\n",
            "#806 Loss: nan\n",
            "#807 Loss: nan\n",
            "#808 Loss: nan\n",
            "#809 Loss: nan\n",
            "#810 Loss: nan\n",
            "#811 Loss: nan\n",
            "#812 Loss: nan\n",
            "#813 Loss: nan\n",
            "#814 Loss: nan\n",
            "#815 Loss: nan\n",
            "#816 Loss: nan\n",
            "#817 Loss: nan\n",
            "#818 Loss: nan\n",
            "#819 Loss: nan\n",
            "#820 Loss: nan\n",
            "#821 Loss: nan\n",
            "#822 Loss: nan\n",
            "#823 Loss: nan\n",
            "#824 Loss: nan\n",
            "#825 Loss: nan\n",
            "#826 Loss: nan\n",
            "#827 Loss: nan\n",
            "#828 Loss: nan\n",
            "#829 Loss: nan\n",
            "#830 Loss: nan\n",
            "#831 Loss: nan\n",
            "#832 Loss: nan\n",
            "#833 Loss: nan\n",
            "#834 Loss: nan\n",
            "#835 Loss: nan\n",
            "#836 Loss: nan\n",
            "#837 Loss: nan\n",
            "#838 Loss: nan\n",
            "#839 Loss: nan\n",
            "#840 Loss: nan\n",
            "#841 Loss: nan\n",
            "#842 Loss: nan\n",
            "#843 Loss: nan\n",
            "#844 Loss: nan\n",
            "#845 Loss: nan\n",
            "#846 Loss: nan\n",
            "#847 Loss: nan\n",
            "#848 Loss: nan\n",
            "#849 Loss: nan\n",
            "#850 Loss: nan\n",
            "#851 Loss: nan\n",
            "#852 Loss: nan\n",
            "#853 Loss: nan\n",
            "#854 Loss: nan\n",
            "#855 Loss: nan\n",
            "#856 Loss: nan\n",
            "#857 Loss: nan\n",
            "#858 Loss: nan\n",
            "#859 Loss: nan\n",
            "#860 Loss: nan\n",
            "#861 Loss: nan\n",
            "#862 Loss: nan\n",
            "#863 Loss: nan\n",
            "#864 Loss: nan\n",
            "#865 Loss: nan\n",
            "#866 Loss: nan\n",
            "#867 Loss: nan\n",
            "#868 Loss: nan\n",
            "#869 Loss: nan\n",
            "#870 Loss: nan\n",
            "#871 Loss: nan\n",
            "#872 Loss: nan\n",
            "#873 Loss: nan\n",
            "#874 Loss: nan\n",
            "#875 Loss: nan\n",
            "#876 Loss: nan\n",
            "#877 Loss: nan\n",
            "#878 Loss: nan\n",
            "#879 Loss: nan\n",
            "#880 Loss: nan\n",
            "#881 Loss: nan\n",
            "#882 Loss: nan\n",
            "#883 Loss: nan\n",
            "#884 Loss: nan\n",
            "#885 Loss: nan\n",
            "#886 Loss: nan\n",
            "#887 Loss: nan\n",
            "#888 Loss: nan\n",
            "#889 Loss: nan\n",
            "#890 Loss: nan\n",
            "#891 Loss: nan\n",
            "#892 Loss: nan\n",
            "#893 Loss: nan\n",
            "#894 Loss: nan\n",
            "#895 Loss: nan\n",
            "#896 Loss: nan\n",
            "#897 Loss: nan\n",
            "#898 Loss: nan\n",
            "#899 Loss: nan\n",
            "#900 Loss: nan\n",
            "#901 Loss: nan\n",
            "#902 Loss: nan\n",
            "#903 Loss: nan\n",
            "#904 Loss: nan\n",
            "#905 Loss: nan\n",
            "#906 Loss: nan\n",
            "#907 Loss: nan\n",
            "#908 Loss: nan\n",
            "#909 Loss: nan\n",
            "#910 Loss: nan\n",
            "#911 Loss: nan\n",
            "#912 Loss: nan\n",
            "#913 Loss: nan\n",
            "#914 Loss: nan\n",
            "#915 Loss: nan\n",
            "#916 Loss: nan\n",
            "#917 Loss: nan\n",
            "#918 Loss: nan\n",
            "#919 Loss: nan\n",
            "#920 Loss: nan\n",
            "#921 Loss: nan\n",
            "#922 Loss: nan\n",
            "#923 Loss: nan\n",
            "#924 Loss: nan\n",
            "#925 Loss: nan\n",
            "#926 Loss: nan\n",
            "#927 Loss: nan\n",
            "#928 Loss: nan\n",
            "#929 Loss: nan\n",
            "#930 Loss: nan\n",
            "#931 Loss: nan\n",
            "#932 Loss: nan\n",
            "#933 Loss: nan\n",
            "#934 Loss: nan\n",
            "#935 Loss: nan\n",
            "#936 Loss: nan\n",
            "#937 Loss: nan\n",
            "#938 Loss: nan\n",
            "#939 Loss: nan\n",
            "#940 Loss: nan\n",
            "#941 Loss: nan\n",
            "#942 Loss: nan\n",
            "#943 Loss: nan\n",
            "#944 Loss: nan\n",
            "#945 Loss: nan\n",
            "#946 Loss: nan\n",
            "#947 Loss: nan\n",
            "#948 Loss: nan\n",
            "#949 Loss: nan\n",
            "#950 Loss: nan\n",
            "#951 Loss: nan\n",
            "#952 Loss: nan\n",
            "#953 Loss: nan\n",
            "#954 Loss: nan\n",
            "#955 Loss: nan\n",
            "#956 Loss: nan\n",
            "#957 Loss: nan\n",
            "#958 Loss: nan\n",
            "#959 Loss: nan\n",
            "#960 Loss: nan\n",
            "#961 Loss: nan\n",
            "#962 Loss: nan\n",
            "#963 Loss: nan\n",
            "#964 Loss: nan\n",
            "#965 Loss: nan\n",
            "#966 Loss: nan\n",
            "#967 Loss: nan\n",
            "#968 Loss: nan\n",
            "#969 Loss: nan\n",
            "#970 Loss: nan\n",
            "#971 Loss: nan\n",
            "#972 Loss: nan\n",
            "#973 Loss: nan\n",
            "#974 Loss: nan\n",
            "#975 Loss: nan\n",
            "#976 Loss: nan\n",
            "#977 Loss: nan\n",
            "#978 Loss: nan\n",
            "#979 Loss: nan\n",
            "#980 Loss: nan\n",
            "#981 Loss: nan\n",
            "#982 Loss: nan\n",
            "#983 Loss: nan\n",
            "#984 Loss: nan\n",
            "#985 Loss: nan\n",
            "#986 Loss: nan\n",
            "#987 Loss: nan\n",
            "#988 Loss: nan\n",
            "#989 Loss: nan\n",
            "#990 Loss: nan\n",
            "#991 Loss: nan\n",
            "#992 Loss: nan\n",
            "#993 Loss: nan\n",
            "#994 Loss: nan\n",
            "#995 Loss: nan\n",
            "#996 Loss: nan\n",
            "#997 Loss: nan\n",
            "#998 Loss: nan\n",
            "#999 Loss: nan\n",
            "Predicted data based on trained weights :\n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([nan])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}